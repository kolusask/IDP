{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from config import *\n",
    "from typing import Tuple\n",
    "\n",
    "from data.util import split_weekdays_and_weekends\n",
    "\n",
    "from prediction_models.dbn import *\n",
    "from prediction_models.kelm import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBN_HIDDEN_LAYER_SIZES = CONFIG['DBN_HIDDEN_LAYER_SIZES']\n",
    "GIBBS_SAMPLING_STEPS = CONFIG['GIBBS_SAMPLING_STEPS']\n",
    "READ_START_DATE = datetime.strptime(CONFIG['READ_START_DATE'], DATE_FORMAT)\n",
    "READ_END_DATE = datetime.strptime(CONFIG['READ_END_DATE'], DATE_FORMAT)\n",
    "TRAIN_START_DATE = datetime.strptime(CONFIG['TRAIN_START_DATE'], DATE_FORMAT)\n",
    "TRAIN_END_DATE = datetime.strptime(CONFIG['TRAIN_END_DATE'], DATE_FORMAT)\n",
    "TRAIN_TIME_WINDOW_SIZE = CONFIG['TRAIN_TIME_WINDOW_SIZE']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the matrix $C$ constructed from $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 347])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_c = torch.load(out_path('mat_c.pt'))\n",
    "TIME_DIM, SPACE_DIM = mat_c.shape\n",
    "mat_c.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split $C$ into workdays and weekends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2112, 347]), torch.Size([768, 347]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_c_wd, mat_c_we = split_weekdays_and_weekends(mat_c, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "assert mat_c_wd.shape[1] == mat_c_we.shape[1] == mat_c.shape[1]\n",
    "assert mat_c_wd.shape[0] + mat_c_we.shape[0] == mat_c.shape[0]\n",
    "mat_c_wd.shape, mat_c_we.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset for pre-training RBMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BinaryVectorDataset(Dataset):\n",
    "    def __init__(self, n_bits: int):\n",
    "        self.n_bits = n_bits\n",
    "        self.format_str = f'{{0:0{self.n_bits}b}}'\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return 200\n",
    "    \n",
    "    def __getitem__(self, index: int) \\\n",
    "            -> Tuple[torch.TensorType, torch.TensorType]:\n",
    "        bin_str = self.format_str.format(index)\n",
    "        return torch.tensor([float(c) for c in bin_str]), torch.tensor([index,])\n",
    "\n",
    "\n",
    "class RandomBinaryVectorDataset(Dataset):\n",
    "    def __init__(self, n_samples, bits_per_sample):\n",
    "        self.n_samples = n_samples\n",
    "        self.bits_per_sample = bits_per_sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.TensorType:\n",
    "        return (torch.rand(self.bits_per_sample) > 0.5).to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "dbn_pre_train_loader = DataLoader(RandomBinaryVectorDataset(10000, SPACE_DIM), batch_size=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train RBMs inside DBN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train to reconstruct all possible combinations of 0's and 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    }
   ],
   "source": [
    "dbn = DBN(SPACE_DIM, DBN_HIDDEN_LAYER_SIZES, k=GIBBS_SAMPLING_STEPS)\n",
    "pre_train_dbn(dbn, dbn_pre_train_loader, print_every=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train prediction of the first section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batch2 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m label_window \u001b[39m=\u001b[39m mat_c[start_time \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m:end_time \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m][:, \u001b[39m0\u001b[39m] \u001b[39m# [60]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m features \u001b[39m=\u001b[39m dbn(train_window) \u001b[39m# [60, 12]\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m kelm\u001b[39m.\u001b[39;49mfit(features, label_window)\n\u001b[1;32m      8\u001b[0m predictions \u001b[39m=\u001b[39m kelm(features) \u001b[39m# [60, 60]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(predictions\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/TUM/IDP/IDP/src/prediction_models/kelm.py:16\u001b[0m, in \u001b[0;36mKELM.fit\u001b[0;34m(self, X, y, reg_coeff, gamma, clean)\u001b[0m\n\u001b[1;32m     13\u001b[0m X_tile \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtile(X, (\u001b[39m1\u001b[39m, X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mX\u001b[39m.\u001b[39mshape, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m omega \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mgamma \u001b[39m*\u001b[39m (X_tile \u001b[39m-\u001b[39m X_tile\u001b[39m.\u001b[39mswapaxes(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m))\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_beta \u001b[39m=\u001b[39m \\\n\u001b[0;32m---> 16\u001b[0m     torch\u001b[39m.\u001b[39;49mbmm(\n\u001b[1;32m     17\u001b[0m         torch\u001b[39m.\u001b[39;49minverse((torch\u001b[39m.\u001b[39;49meye(X\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]) \u001b[39m/\u001b[39;49m reg_coeff) \u001b[39m+\u001b[39;49m omega),\n\u001b[1;32m     18\u001b[0m         y[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m]\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m X\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batch2 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "kelm = KELM()\n",
    "for start_time in range(TIME_DIM - TRAIN_TIME_WINDOW_SIZE - 1):\n",
    "    end_time = start_time + TRAIN_TIME_WINDOW_SIZE\n",
    "    train_window = mat_c[start_time:end_time] # [60, 12]\n",
    "    label_window = mat_c[start_time + 1:end_time + 1][:, 0] # [60]\n",
    "    features = dbn(train_window) # [60, 12]\n",
    "    kelm.fit(features, label_window)\n",
    "    predictions = kelm(features) # [60, 60]\n",
    "    print(predictions.shape)\n",
    "    print(loss_fn(predictions, label_window))\n",
    "    # USE THIS TO TRAIN DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
