{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from config import *\n",
    "\n",
    "from data.util import split_weekdays_and_weekends, crop_q_between_dates\n",
    "\n",
    "from prediction_models.dbn import *\n",
    "from prediction_models.kelm import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBN_LAYER_SIZES = CONFIG['DBN_LAYER_SIZES']\n",
    "DBN_HIDDEN_LAYER_SIZES = CONFIG['DBN_HIDDEN_LAYER_SIZES']\n",
    "GIBBS_SAMPLING_STEPS = CONFIG['GIBBS_SAMPLING_STEPS']\n",
    "READ_START_DATE = datetime.strptime(CONFIG['READ_START_DATE'], DATE_FORMAT)\n",
    "READ_END_DATE = datetime.strptime(CONFIG['READ_END_DATE'], DATE_FORMAT)\n",
    "TRAIN_START_DATE = datetime.strptime(CONFIG['TRAIN_START_DATE'], DATE_FORMAT)\n",
    "TRAIN_END_DATE = datetime.strptime(CONFIG['TRAIN_END_DATE'], DATE_FORMAT)\n",
    "TRAIN_TIME_WINDOW_SIZE = CONFIG['TRAIN_TIME_WINDOW_SIZE']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train RBMs inside DBN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train to reconstruct all possible combinations of 0's and 1's"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryVectorDataset(Dataset):\n",
    "    def __init__(self, n_bits: int):\n",
    "        self.n_bits = n_bits\n",
    "        self.format_str = f'{{0:0{self.n_bits}b}}'\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return 2 ** self.n_bits\n",
    "    \n",
    "    def __getitem__(self, index: int) \\\n",
    "            -> Tuple[torch.TensorType, torch.TensorType]:\n",
    "        bin_str = self.format_str.format(index)\n",
    "        return torch.tensor([float(c) for c in bin_str]), torch.tensor([index,])\n",
    "\n",
    "dbn_pre_train_loader = DataLoader(BinaryVectorDataset(INPUT_SIZE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and pre-train DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Machine 0:\tLoss: -1.9836238622665405\n",
      "Epoch 0, Machine 1:\tLoss: -0.3505793809890747\n",
      "Epoch 0, Machine 2:\tLoss: -0.1488177478313446\n",
      "Epoch 1, Machine 0:\tLoss: -2.5248847007751465\n",
      "Epoch 1, Machine 1:\tLoss: -0.44476449489593506\n",
      "Epoch 1, Machine 2:\tLoss: -0.6324977874755859\n",
      "Epoch 2, Machine 0:\tLoss: -2.6716198921203613\n",
      "Epoch 2, Machine 1:\tLoss: -0.5382086038589478\n",
      "Epoch 2, Machine 2:\tLoss: -0.7409056425094604\n",
      "Epoch 3, Machine 0:\tLoss: -2.728278875350952\n",
      "Epoch 3, Machine 1:\tLoss: -0.5518969297409058\n",
      "Epoch 3, Machine 2:\tLoss: -0.8258986473083496\n",
      "Epoch 4, Machine 0:\tLoss: -2.7757086753845215\n",
      "Epoch 4, Machine 1:\tLoss: -0.5673878192901611\n",
      "Epoch 4, Machine 2:\tLoss: -0.9229025840759277\n",
      "Epoch 5, Machine 0:\tLoss: -2.8171348571777344\n",
      "Epoch 5, Machine 1:\tLoss: -0.6019374132156372\n",
      "Epoch 5, Machine 2:\tLoss: -0.9747148752212524\n",
      "Epoch 6, Machine 0:\tLoss: -2.8436872959136963\n",
      "Epoch 6, Machine 1:\tLoss: -0.6580054759979248\n",
      "Epoch 6, Machine 2:\tLoss: -0.9391961097717285\n",
      "Epoch 7, Machine 0:\tLoss: -2.8598694801330566\n",
      "Epoch 7, Machine 1:\tLoss: -0.6784170866012573\n",
      "Epoch 7, Machine 2:\tLoss: -0.9099304676055908\n",
      "Epoch 8, Machine 0:\tLoss: -2.8700525760650635\n",
      "Epoch 8, Machine 1:\tLoss: -0.679558277130127\n",
      "Epoch 8, Machine 2:\tLoss: -0.9342827796936035\n",
      "Epoch 9, Machine 0:\tLoss: -2.8762664794921875\n",
      "Epoch 9, Machine 1:\tLoss: -0.6862380504608154\n",
      "Epoch 9, Machine 2:\tLoss: -0.9459492564201355\n",
      "Epoch 10, Machine 0:\tLoss: -2.8855743408203125\n",
      "Epoch 10, Machine 1:\tLoss: -0.6940314173698425\n",
      "Epoch 10, Machine 2:\tLoss: -0.9408299922943115\n",
      "Epoch 11, Machine 0:\tLoss: -2.9013373851776123\n",
      "Epoch 11, Machine 1:\tLoss: -0.6896882057189941\n",
      "Epoch 11, Machine 2:\tLoss: -0.9319601655006409\n",
      "Epoch 12, Machine 0:\tLoss: -2.9276461601257324\n",
      "Epoch 12, Machine 1:\tLoss: -0.6524261236190796\n",
      "Epoch 12, Machine 2:\tLoss: -0.9239528179168701\n",
      "Epoch 13, Machine 0:\tLoss: -2.948052167892456\n",
      "Epoch 13, Machine 1:\tLoss: -0.5932166576385498\n",
      "Epoch 13, Machine 2:\tLoss: -0.9012948274612427\n",
      "Epoch 14, Machine 0:\tLoss: -2.96989369392395\n",
      "Epoch 14, Machine 1:\tLoss: -0.5577154159545898\n",
      "Epoch 14, Machine 2:\tLoss: -0.9037688970565796\n",
      "Epoch 15, Machine 0:\tLoss: -2.9839775562286377\n",
      "Epoch 15, Machine 1:\tLoss: -0.5046495199203491\n",
      "Epoch 15, Machine 2:\tLoss: -0.9022549986839294\n",
      "Epoch 16, Machine 0:\tLoss: -2.9892396926879883\n",
      "Epoch 16, Machine 1:\tLoss: -0.4945833086967468\n",
      "Epoch 16, Machine 2:\tLoss: -0.8979450464248657\n",
      "Epoch 17, Machine 0:\tLoss: -2.9888229370117188\n",
      "Epoch 17, Machine 1:\tLoss: -0.4964596927165985\n",
      "Epoch 17, Machine 2:\tLoss: -0.8567074537277222\n",
      "Epoch 18, Machine 0:\tLoss: -2.987679958343506\n",
      "Epoch 18, Machine 1:\tLoss: -0.5053804516792297\n",
      "Epoch 18, Machine 2:\tLoss: -0.8620028495788574\n",
      "Epoch 19, Machine 0:\tLoss: -2.9857382774353027\n",
      "Epoch 19, Machine 1:\tLoss: -0.5114054679870605\n",
      "Epoch 19, Machine 2:\tLoss: -0.8672225475311279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DBN(\n",
       "  (rbms): ModuleList(\n",
       "    (0-2): 3 x RBM()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbn = DBN(INPUT_SIZE, HIDDEN_LAYER_SIZES, k=GIBBS_SAMPLING_STEPS)\n",
    "pre_train_dbm(dbn, dbn_pre_train_loader, print_every=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the matrix $E_t$ constructed from $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 472])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_q_resid = torch.load(out_path('mat_q_resid.pt'))\n",
    "mat_q_resid = crop_q_between_dates(mat_q_resid, READ_START_DATE, READ_END_DATE, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "mat_q_resid.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split $Q$ into workdays and weekends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2112, 472]), torch.Size([768, 472]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_q_wd, mat_q_we = split_weekdays_and_weekends(mat_q, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "assert mat_q_wd.shape[1] == mat_q_we.shape[1] == mat_q.shape[1]\n",
    "assert mat_q_wd.shape[0] + mat_q_we.shape[0] == mat_q.shape[0]\n",
    "mat_q_wd.shape, mat_q_we.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train prediction of the first section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
