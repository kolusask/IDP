{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics as tm\n",
    "\n",
    "from config import *\n",
    "from stages import *\n",
    "from train import *\n",
    "from data.ssa import SSA\n",
    "\n",
    "from data.util import crop_q_between, split_weekdays_and_weekends\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  4.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  1., 16.,  ..., 29.,  0.,  0.],\n",
       "        [ 0.,  0.,  9.,  ..., 16.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = tm.MeanSquaredError().to(CONFIG.device)\n",
    "mape = tm.MeanAbsolutePercentageError().to(CONFIG.device)\n",
    "mat_q = CONFIG.load('mat_q.pt')\n",
    "CONFIG.alpha = 1\n",
    "CONFIG.device\n",
    "mat_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare MSE and MAPE losses for different parameter configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-train DBN and Train DBN attaching KELM on each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 12500 4 torch.Size([2016, 1])\n",
      "Epoch 0:\n",
      "\tRBM 0: Loss=-17.420974731445312\n",
      "\tRBM 1: Loss=0.6578211784362793\n",
      "\tRBM 2: Loss=0.22304177284240723\n",
      "Epoch 5:\n",
      "\tRBM 0: Loss=-24.367263793945312\n",
      "\tRBM 1: Loss=0.029702186584472656\n",
      "\tRBM 2: Loss=-0.11160850524902344\n",
      "Epoch 10:\n",
      "\tRBM 0: Loss=-31.273244857788086\n",
      "\tRBM 1: Loss=-0.43567895889282227\n",
      "\tRBM 2: Loss=-0.3647644519805908\n",
      "Epoch 15:\n",
      "\tRBM 0: Loss=-38.08616256713867\n",
      "\tRBM 1: Loss=-0.7163071632385254\n",
      "\tRBM 2: Loss=-0.5539155006408691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [00:06<10:37,  1.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/askar/TUM/IDP/IDP/04_model_training.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/askar/TUM/IDP/IDP/04_model_training.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(gamma, reg_coeff, P, N, mat_c\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/askar/TUM/IDP/IDP/04_model_training.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdel\u001b[39;00m mat_c\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/askar/TUM/IDP/IDP/04_model_training.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m dbn, kelm, val_resid_we_dataloader \u001b[39m=\u001b[39m train_with_config(CONFIG, mat_c_we_datasets, dbn_training_epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, gamma\u001b[39m=\u001b[39mgamma, reg_coeff\u001b[39m=\u001b[39mreg_coeff)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/askar/TUM/IDP/IDP/04_model_training.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m mse_loss_trend \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0.\u001b[39m,])\u001b[39m.\u001b[39mto(CONFIG\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/askar/TUM/IDP/IDP/04_model_training.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m mse_loss_resid \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0.\u001b[39m,])\u001b[39m.\u001b[39mto(CONFIG\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/TUM/IDP/IDP/src/train.py:210\u001b[0m, in \u001b[0;36mtrain_with_config\u001b[0;34m(config, datasets, dbn_training_epochs, dbn_eval_each, stride, gamma, reg_coeff)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mfor\u001b[39;00m dbn_epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(dbn_training_epochs)):\n\u001b[1;32m    209\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 210\u001b[0m     train_loss: torch\u001b[39m.\u001b[39mFloatTensor \u001b[39m=\u001b[39m epoch(\n\u001b[1;32m    211\u001b[0m         dbn, kelm, train_dataloader, mse, config\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m     train_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    214\u001b[0m     optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/TUM/IDP/IDP/src/train.py:168\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(dbn, kelm, dataloader, loss_fn, device)\u001b[0m\n\u001b[1;32m    165\u001b[0m     pred \u001b[39m=\u001b[39m dbn(X)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    166\u001b[0m     pred \u001b[39m=\u001b[39m kelm(pred)\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 168\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m loss \u001b[39m/\u001b[39m n_samples\n",
      "File \u001b[0;32m~/miniconda3/envs/.idp/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/.idp/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/.idp/lib/python3.11/site-packages/torchmetrics/metric.py:236\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_full_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_reduce_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/miniconda3/envs/.idp/lib/python3.11/site-packages/torchmetrics/metric.py:292\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m global_state \u001b[39m=\u001b[39m {attr: \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr) \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_defaults\u001b[39m.\u001b[39mkeys()}\n\u001b[1;32m    291\u001b[0m _update_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_count\n\u001b[0;32m--> 292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    294\u001b[0m \u001b[39m# local syncronization settings\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_sync \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_sync_on_step\n",
      "File \u001b[0;32m~/miniconda3/envs/.idp/lib/python3.11/site-packages/torchmetrics/metric.py:557\u001b[0m, in \u001b[0;36mMetric.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m current_val \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n\u001b[1;32m    556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(default, Tensor):\n\u001b[0;32m--> 557\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, attr, default\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mto(current_val\u001b[39m.\u001b[39mdevice))\n\u001b[1;32m    558\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, attr, [])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_wd = {}\n",
    "results_we = {}\n",
    "\n",
    "ssa = SSA(4, [2, 2], CONFIG.device)\n",
    "\n",
    "CONFIG.spectral_threshold = 1440\n",
    "CONFIG.dbn_hidden_layer_sizes = [80, 80, 80]\n",
    "\n",
    "gamma = 1\n",
    "reg_coeff = 1\n",
    "\n",
    "SECTION_INDEX = 0\n",
    "\n",
    "for gamma in [gamma]:#range(1, 26, 5):\n",
    "  for reg_coeff in [reg_coeff]:#range(1, 102, 25):\n",
    "    for P in range(3125 * 4, 25001, 3125):\n",
    "        CONFIG.spectral_threshold = P\n",
    "        for N in range(4, 324, 80):\n",
    "          CONFIG.dbn_hidden_layer_sizes = [N, N, N]\n",
    "\n",
    "          mat_q_trend, mat_q_resid = preprocess_data(CONFIG.spectral_threshold, mat_q)\n",
    "          mat_c, mat_x, nonempty, representatives = compress_data(\n",
    "              mat_q_resid.abs(), CONFIG.read_period, CONFIG.train_period, CONFIG.alpha)\n",
    "          # while mat_c.shape[1] == 1:\n",
    "          #   CONFIG.alpha += 0.01\n",
    "          #   print(f'alpha={CONFIG.alpha}')\n",
    "          #   mat_c, mat_x, nonempty, representatives = compress_data(\n",
    "          #     mat_q_resid.abs(), CONFIG.read_period, CONFIG.train_period, CONFIG.alpha)\n",
    "          mat_q_trend = mat_q_trend[:, nonempty][:, representatives].abs()\n",
    "          mat_q_trend = mat_q_trend[:, SECTION_INDEX][:, None]\n",
    "          mat_c = mat_c[:, SECTION_INDEX][:, None]\n",
    "\n",
    "          (_, val_trend_wd_dataloader, _), (train_trend_we_dataloader, val_trend_we_dataloader, _) = crop_and_split_mat(mat_q_trend, CONFIG)\n",
    "          # (_, val_resid_wd_dataloader, _), (_, val_resid_we_dataloader, _) = crop_and_split_mat(mat_q_resid, CONFIG)\n",
    "          mat_c_wd_datasets, mat_c_we_datasets = split_mat(mat_c, CONFIG)\n",
    "          print(gamma, reg_coeff, P, N, mat_c.shape)\n",
    "\n",
    "          del mat_c\n",
    "\n",
    "          dbn, kelm, val_resid_we_dataloader = train_with_config(CONFIG, mat_c_we_datasets, dbn_training_epochs=100, stride=1, gamma=gamma, reg_coeff=reg_coeff)\n",
    "\n",
    "          mse_loss_trend = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          mse_loss_resid = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          mse_loss_overall = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          iter_trend = iter(val_trend_we_dataloader)\n",
    "          n_samples = 0\n",
    "          for X_resid, y_resid in val_resid_we_dataloader:\n",
    "              X_trend, y_trend = next(iter_trend)\n",
    "              pred_trend = ssa.forecast(X_trend.squeeze().T, 1).sum(0)[-1][None]\n",
    "              mse_loss_trend += mse(pred_trend, y_trend)\n",
    "              # print('TREND', y_trend, pred_trend)\n",
    "\n",
    "              pred_resid = dbn(X_resid).squeeze()\n",
    "              pred_resid = kelm(pred_resid).T\n",
    "              mse_loss_resid += mse(pred_resid, y_resid)\n",
    "              # print('RESID', y_resid, pred_resid)\n",
    "\n",
    "              pred = pred_trend + pred_resid\n",
    "\n",
    "              y = y_trend + y_resid\n",
    "              # print('OVRLL', y, pred)\n",
    "              # print('=====')\n",
    "\n",
    "              mse_loss_overall += mse(pred, y)\n",
    "              n_samples += 1\n",
    "\n",
    "          mse_loss_trend /= n_samples\n",
    "          mse_loss_resid /= n_samples\n",
    "          mse_loss_overall /= n_samples\n",
    "\n",
    "          print(f'gamma={gamma}, reg_coeff={reg_coeff}, WE P={CONFIG.spectral_threshold}, N={CONFIG.dbn_hidden_layer_sizes}, Loss_trend={mse_loss_trend.item()}, Loss_resid={mse_loss_resid.item()}, Loss_overall={mse_loss_overall.item()}')\n",
    "          del dbn\n",
    "          del kelm\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
