{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics as tm\n",
    "\n",
    "from json import load, dump\n",
    "\n",
    "from config import *\n",
    "from stages import *\n",
    "from train import *\n",
    "from data.ssa import SSA\n",
    "\n",
    "from data.util import crop_q_between, split_weekdays_and_weekends\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tm.MeanSquaredError().to(CONFIG.device)\n",
    "mape = tm.MeanAbsolutePercentageError().to(CONFIG.device)\n",
    "mat_q = CONFIG.load('mat_q.pt')\n",
    "CONFIG.alpha = 0.2\n",
    "CONFIG.alpha = 1000\n",
    "mat_c_all, mat_x_repr, nonempty, representatives = compress_data(\n",
    "    mat_q.abs(), CONFIG.read_period, CONFIG.train_period, CONFIG.alpha)\n",
    "mat_q = mat_q[:, mat_q.sum(dim=0) > 0]\n",
    "normalizing_constants_all = mat_q.max(dim=0)[0]\n",
    "mat_q /= normalizing_constants_all\n",
    "mat_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.save(mat_q, 'mat_q_nonzero.pt')\n",
    "mat_q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare MSE and MAPE losses for different parameter configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-train DBN and Train DBN attaching KELM on each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_wd = {}\n",
    "results_we = {}\n",
    "\n",
    "ssa = SSA(4, [2, 2], CONFIG.device)\n",
    "\n",
    "CONFIG.spectral_threshold = 1440\n",
    "CONFIG.dbn_hidden_layer_sizes = [80, 80, 80]\n",
    "\n",
    "gamma = 1\n",
    "reg_coeff = 1\n",
    "\n",
    "\n",
    "for P in tqdm(range(110, 210, 10)):#[40, 126.5, 400, 1265, 4000, 12650, 40000]):\n",
    "  results_P = {}\n",
    "  CONFIG.spectral_threshold = P\n",
    "  for N in range(4, 324, 80):\n",
    "    CONFIG.dbn_hidden_layer_sizes = [N, N, N]\n",
    "\n",
    "    mat_c_all, mat_x_all, nonempty, representatives = compress_data(\n",
    "        mat_q.abs(), CONFIG.read_period, CONFIG.train_period, CONFIG.alpha)\n",
    "    mat_q_trend_all, mat_q_resid_all = preprocess_data(\n",
    "        CONFIG.spectral_threshold, mat_c_all)\n",
    "    mat_q_resid_all = crop_q_between(mat_q_resid_all, CONFIG.read_period,\n",
    "                                      CONFIG.train_period).real\n",
    "    mat_c_all = crop_q_between(\n",
    "        mat_c_all, CONFIG.read_period, CONFIG.train_period)\n",
    "    \n",
    "    mat_q_trend_all\n",
    "\n",
    "    for section_index, section_number in tqdm(enumerate(representatives.tolist())):\n",
    "      mat_q_trend = mat_q_trend_all[:, section_index][:, None].real\n",
    "      mat_q_resid = mat_q_resid_all[:, section_index][:, None].real\n",
    "\n",
    "      norm_const = normalizing_constants_all[section_number].item()\n",
    "\n",
    "      (_, val_trend_wd_dataloader, _), (_, val_trend_we_dataloader, _) = \\\n",
    "        crop_and_split_mat(mat_q_trend, CONFIG, separate_weekends=False)\n",
    "      mat_c_wd_datasets, mat_c_we_datasets = split_mat(mat_q_resid, CONFIG)\n",
    "\n",
    "      del mat_q_resid\n",
    "\n",
    "      dbn, kelm, val_resid_wd_dataloader = train_with_config(\n",
    "        CONFIG, mat_c_wd_datasets, dbn_training_epochs=100, stride=1,\n",
    "        gamma=gamma, reg_coeff=reg_coeff\n",
    "      )\n",
    "      prefix = f'alpha{CONFIG.alpha}_p{P}_n{N}_sec{section_number}_'\\\n",
    "        .replace('.', '_')\n",
    "      CONFIG.save(dbn.state_dict(), prefix + 'dbn.pt')\n",
    "      CONFIG.save(kelm.state_dict(), prefix + 'kelm.pt')\n",
    "\n",
    "      mse_loss_trend = torch.tensor([0.,]).to(CONFIG.device)\n",
    "      mse_loss_resid = torch.tensor([0.,]).to(CONFIG.device)\n",
    "      mse_loss_overall = torch.tensor([0.,]).to(CONFIG.device)\n",
    "      mse_loss_trend_unnorm = torch.tensor([0.,]).to(CONFIG.device)\n",
    "      mse_loss_resid_unnorm = torch.tensor([0.,]).to(CONFIG.device)\n",
    "      mse_loss_overall_unnorm = torch.tensor([0.,]).to(CONFIG.device)\n",
    "\n",
    "      trend_y = torch.tensor([0.,]).to(CONFIG.device)\n",
    "      resid_y = torch.tensor([0.,]).to(CONFIG.device)\n",
    "      overall_y = torch.tensor([0.,]).to(CONFIG.device)\n",
    "\n",
    "      iter_trend = iter(val_trend_wd_dataloader)\n",
    "      n_samples = 0\n",
    "\n",
    "      resid = {'y': [], 'pred': []}\n",
    "      trend = {'y': [], 'pred': []}\n",
    "      overall = {'y': [], 'pred': []}\n",
    "      for X_resid, y_resid in val_resid_wd_dataloader:\n",
    "          resid_y += y_resid.item()\n",
    "          X_trend, y_trend = next(iter_trend)\n",
    "          trend_y += y_trend.item()\n",
    "          pred_trend = ssa.forecast(\n",
    "              X_trend.squeeze(0).T, 1).sum(0)[-1][None]\n",
    "          mse_loss_trend += mse(pred_trend, y_trend)\n",
    "          mse_loss_trend_unnorm += mse(pred_trend * norm_const, y_trend * norm_const)\n",
    "          trend['y'].append(y_trend.item() * norm_const)\n",
    "          trend['pred'].append(pred_trend.item() * norm_const)\n",
    "\n",
    "          pred_resid = dbn(X_resid).squeeze(0)\n",
    "          pred_resid = kelm(pred_resid).T\n",
    "          mse_loss_resid += mse(pred_resid, y_resid)\n",
    "          mse_loss_resid_unnorm += mse(pred_resid * norm_const, y_resid * norm_const)\n",
    "          resid['y'].append(y_resid.item() * norm_const)\n",
    "          resid['pred'].append(pred_resid.item() * norm_const)\n",
    "\n",
    "          pred = pred_trend + pred_resid\n",
    "\n",
    "          y = y_trend + y_resid\n",
    "          overall_y += y.item()\n",
    "          overall['y'].append(y.item() * norm_const)\n",
    "          overall['pred'].append(pred.item() * norm_const)\n",
    "\n",
    "          mse_loss_overall += mse(pred, y).item()\n",
    "          mse_loss_overall_unnorm += mse(pred * norm_const, y * norm_const).item()\n",
    "          n_samples += 1\n",
    "\n",
    "      mse_loss_trend /= n_samples\n",
    "      mse_loss_resid /= n_samples\n",
    "      mse_loss_overall /= n_samples\n",
    "      mse_loss_resid_unnorm /= n_samples\n",
    "      mse_loss_trend_unnorm /= n_samples\n",
    "      mse_loss_overall_unnorm /= n_samples\n",
    "\n",
    "      resid_y /= n_samples\n",
    "      trend_y /= n_samples\n",
    "      overall_y /= n_samples\n",
    "\n",
    "      results_P[section_number] = {\n",
    "          'resid': resid,\n",
    "          'trend': trend,\n",
    "          'overall': overall\n",
    "      }\n",
    "\n",
    "      with open(f'results_wd_{section_number}.json', 'w') as file:\n",
    "        dump(results_P[section_number], file)\n",
    "\n",
    "      del dbn\n",
    "      del kelm\n",
    "\n",
    "  results_wd[P] = results_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results_wd_norm_adjust.json', 'w') as file:\n",
    "  json.dump(results_we, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
