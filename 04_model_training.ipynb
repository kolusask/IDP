{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from config import *\n",
    "from typing import Tuple\n",
    "\n",
    "from data.util import split_weekdays_and_weekends\n",
    "\n",
    "from prediction_models.dbn import *\n",
    "from prediction_models.kelm import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBN_HIDDEN_LAYER_SIZES = CONFIG['DBN_HIDDEN_LAYER_SIZES']\n",
    "GIBBS_SAMPLING_STEPS = CONFIG['GIBBS_SAMPLING_STEPS']\n",
    "READ_START_DATE = datetime.strptime(CONFIG['READ_START_DATE'], DATE_FORMAT)\n",
    "READ_END_DATE = datetime.strptime(CONFIG['READ_END_DATE'], DATE_FORMAT)\n",
    "TRAIN_START_DATE = datetime.strptime(CONFIG['TRAIN_START_DATE'], DATE_FORMAT)\n",
    "TRAIN_END_DATE = datetime.strptime(CONFIG['TRAIN_END_DATE'], DATE_FORMAT)\n",
    "TRAIN_TIME_WINDOW_SIZE = CONFIG['TRAIN_TIME_WINDOW_SIZE']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the matrix $C$ constructed from $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2880, 12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_c = torch.load(out_path('mat_c.pt'))\n",
    "TIME_DIM, SPACE_DIM = mat_c.shape\n",
    "mat_c.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split $C$ into workdays and weekends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2112, 12]), torch.Size([768, 12]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_c_wd, mat_c_we = split_weekdays_and_weekends(mat_c, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "assert mat_c_wd.shape[1] == mat_c_we.shape[1] == mat_c.shape[1]\n",
    "assert mat_c_wd.shape[0] + mat_c_we.shape[0] == mat_c.shape[0]\n",
    "mat_c_wd.shape, mat_c_we.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset for pre-training RBMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BinaryVectorDataset(Dataset):\n",
    "    def __init__(self, n_bits: int):\n",
    "        self.n_bits = n_bits\n",
    "        self.format_str = f'{{0:0{self.n_bits}b}}'\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return 2 ** self.n_bits\n",
    "    \n",
    "    def __getitem__(self, index: int) \\\n",
    "            -> Tuple[torch.TensorType, torch.TensorType]:\n",
    "        bin_str = self.format_str.format(index)\n",
    "        return torch.tensor([float(c) for c in bin_str]), torch.tensor([index,])\n",
    "\n",
    "dbn_pre_train_loader = DataLoader(BinaryVectorDataset(SPACE_DIM), batch_size=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train RBMs inside DBN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train to reconstruct all possible combinations of 0's and 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Machine 0:\tLoss: 3.63346529006958\n",
      "Epoch 0, Machine 1:\tLoss: 10.110261917114258\n",
      "Epoch 0, Machine 2:\tLoss: 6.241438865661621\n",
      "Epoch 1, Machine 0:\tLoss: 0.27469363808631897\n",
      "Epoch 1, Machine 1:\tLoss: 3.0161426067352295\n",
      "Epoch 1, Machine 2:\tLoss: 1.9354207515716553\n",
      "Epoch 2, Machine 0:\tLoss: -0.944818377494812\n",
      "Epoch 2, Machine 1:\tLoss: 0.374968558549881\n",
      "Epoch 2, Machine 2:\tLoss: 0.4830241799354553\n",
      "Epoch 3, Machine 0:\tLoss: -1.3790874481201172\n",
      "Epoch 3, Machine 1:\tLoss: -0.6266959309577942\n",
      "Epoch 3, Machine 2:\tLoss: -0.038316428661346436\n",
      "Epoch 4, Machine 0:\tLoss: -1.6058722734451294\n",
      "Epoch 4, Machine 1:\tLoss: -0.7914646863937378\n",
      "Epoch 4, Machine 2:\tLoss: -0.46987029910087585\n",
      "Epoch 5, Machine 0:\tLoss: -1.7674893140792847\n",
      "Epoch 5, Machine 1:\tLoss: -0.8008990287780762\n",
      "Epoch 5, Machine 2:\tLoss: -0.6826756596565247\n",
      "Epoch 6, Machine 0:\tLoss: -1.876910924911499\n",
      "Epoch 6, Machine 1:\tLoss: -0.7868847846984863\n",
      "Epoch 6, Machine 2:\tLoss: -0.7667257785797119\n",
      "Epoch 7, Machine 0:\tLoss: -1.9594792127609253\n",
      "Epoch 7, Machine 1:\tLoss: -0.7684112787246704\n",
      "Epoch 7, Machine 2:\tLoss: -0.8659903407096863\n",
      "Epoch 8, Machine 0:\tLoss: -2.0286061763763428\n",
      "Epoch 8, Machine 1:\tLoss: -0.748572587966919\n",
      "Epoch 8, Machine 2:\tLoss: -0.8463979959487915\n",
      "Epoch 9, Machine 0:\tLoss: -2.0961265563964844\n",
      "Epoch 9, Machine 1:\tLoss: -0.7255854606628418\n",
      "Epoch 9, Machine 2:\tLoss: -0.8902498483657837\n",
      "Epoch 10, Machine 0:\tLoss: -2.1657886505126953\n",
      "Epoch 10, Machine 1:\tLoss: -0.6986335515975952\n",
      "Epoch 10, Machine 2:\tLoss: -0.8700771331787109\n",
      "Epoch 11, Machine 0:\tLoss: -2.2390341758728027\n",
      "Epoch 11, Machine 1:\tLoss: -0.6700275540351868\n",
      "Epoch 11, Machine 2:\tLoss: -0.8992552757263184\n",
      "Epoch 12, Machine 0:\tLoss: -2.313103199005127\n",
      "Epoch 12, Machine 1:\tLoss: -0.6407521367073059\n",
      "Epoch 12, Machine 2:\tLoss: -0.8909667730331421\n",
      "Epoch 13, Machine 0:\tLoss: -2.388739824295044\n",
      "Epoch 13, Machine 1:\tLoss: -0.6111853122711182\n",
      "Epoch 13, Machine 2:\tLoss: -0.9110661149024963\n",
      "Epoch 14, Machine 0:\tLoss: -2.4617021083831787\n",
      "Epoch 14, Machine 1:\tLoss: -0.5877740383148193\n",
      "Epoch 14, Machine 2:\tLoss: -0.9203398823738098\n",
      "Epoch 15, Machine 0:\tLoss: -2.5281171798706055\n",
      "Epoch 15, Machine 1:\tLoss: -0.5681682825088501\n",
      "Epoch 15, Machine 2:\tLoss: -0.9275349378585815\n",
      "Epoch 16, Machine 0:\tLoss: -2.5888874530792236\n",
      "Epoch 16, Machine 1:\tLoss: -0.5553920269012451\n",
      "Epoch 16, Machine 2:\tLoss: -0.9292804598808289\n",
      "Epoch 17, Machine 0:\tLoss: -2.6407344341278076\n",
      "Epoch 17, Machine 1:\tLoss: -0.5474300384521484\n",
      "Epoch 17, Machine 2:\tLoss: -0.9303349256515503\n",
      "Epoch 18, Machine 0:\tLoss: -2.68464994430542\n",
      "Epoch 18, Machine 1:\tLoss: -0.5420278310775757\n",
      "Epoch 18, Machine 2:\tLoss: -0.9258406162261963\n",
      "Epoch 19, Machine 0:\tLoss: -2.7221312522888184\n",
      "Epoch 19, Machine 1:\tLoss: -0.5382052659988403\n",
      "Epoch 19, Machine 2:\tLoss: -0.9208217263221741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DBN(\n",
       "  (rbms): ModuleList(\n",
       "    (0-2): 3 x RBM()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbn = DBN(SPACE_DIM, DBN_HIDDEN_LAYER_SIZES, k=GIBBS_SAMPLING_STEPS)\n",
    "pre_train_dbn(dbn, dbn_pre_train_loader, print_every=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train prediction of the first section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batch2 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m label_window \u001b[39m=\u001b[39m mat_c[start_time \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m:end_time \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m][:, \u001b[39m0\u001b[39m] \u001b[39m# [60]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m features \u001b[39m=\u001b[39m dbn(train_window) \u001b[39m# [60, 12]\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m kelm\u001b[39m.\u001b[39;49mfit(features, label_window)\n\u001b[1;32m      8\u001b[0m predictions \u001b[39m=\u001b[39m kelm(features) \u001b[39m# [60, 60]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(predictions\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/TUM/IDP/IDP/src/prediction_models/kelm.py:16\u001b[0m, in \u001b[0;36mKELM.fit\u001b[0;34m(self, X, y, reg_coeff, gamma, clean)\u001b[0m\n\u001b[1;32m     13\u001b[0m X_tile \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtile(X, (\u001b[39m1\u001b[39m, X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mX\u001b[39m.\u001b[39mshape, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m omega \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mgamma \u001b[39m*\u001b[39m (X_tile \u001b[39m-\u001b[39m X_tile\u001b[39m.\u001b[39mswapaxes(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m))\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_beta \u001b[39m=\u001b[39m \\\n\u001b[0;32m---> 16\u001b[0m     torch\u001b[39m.\u001b[39;49mbmm(\n\u001b[1;32m     17\u001b[0m         torch\u001b[39m.\u001b[39;49minverse((torch\u001b[39m.\u001b[39;49meye(X\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]) \u001b[39m/\u001b[39;49m reg_coeff) \u001b[39m+\u001b[39;49m omega),\n\u001b[1;32m     18\u001b[0m         y[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m]\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m X\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batch2 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "kelm = KELM()\n",
    "for start_time in range(TIME_DIM - TRAIN_TIME_WINDOW_SIZE - 1):\n",
    "    end_time = start_time + TRAIN_TIME_WINDOW_SIZE\n",
    "    train_window = mat_c[start_time:end_time] # [60, 12]\n",
    "    label_window = mat_c[start_time + 1:end_time + 1][:, 0] # [60]\n",
    "    features = dbn(train_window) # [60, 12]\n",
    "    kelm.fit(features, label_window)\n",
    "    predictions = kelm(features) # [60, 60]\n",
    "    print(predictions.shape)\n",
    "    print(loss_fn(predictions, label_window))\n",
    "    # USE THIS TO TRAIN DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
