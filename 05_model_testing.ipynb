{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics as tm\n",
    "\n",
    "from json import load, dump\n",
    "\n",
    "from config import *\n",
    "from stages import *\n",
    "from train import *\n",
    "from data.ssa import SSA\n",
    "\n",
    "from data.util import crop_q_between, split_weekdays_and_weekends\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = CONFIG.data_split[0]\n",
    "mse = tm.MeanSquaredError().to(CONFIG.device)\n",
    "mape = tm.MeanAbsolutePercentageError().to(CONFIG.device)\n",
    "mat_q = CONFIG.load('mat_q.pt')\n",
    "CONFIG.alpha = 0.2\n",
    "CONFIG.alpha = 1000\n",
    "mat_c_all, mat_x_repr, nonempty, representatives = compress_data(\n",
    "    mat_q.abs(), CONFIG.read_period, CONFIG.test_period, CONFIG.alpha)\n",
    "mat_q = mat_q[:, mat_q.sum(dim=0) > 0]\n",
    "normalizing_constants_all = mat_q.max(dim=0)[0]\n",
    "# mat_q /= normalizing_constants_all\n",
    "mat_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_wd = {}\n",
    "results_we = {}\n",
    "\n",
    "ssa = SSA(4, [2, 2], CONFIG.device)\n",
    "\n",
    "CONFIG.spectral_threshold = 1440\n",
    "CONFIG.dbn_hidden_layer_sizes = [80, 80, 80]\n",
    "\n",
    "gamma = 1\n",
    "reg_coeff = 1\n",
    "\n",
    "CONFIG.data_split = [0.0, 1.0]\n",
    "\n",
    "for gamma in [gamma]:#range(1, 26, 5):\n",
    "  for reg_coeff in [reg_coeff]:#range(1, 102, 25):\n",
    "    for P in [70]:#tqdm(range(110, 210, 10)):#[40, 126.5, 400, 1265, 4000, 12650, 40000]):#range(125, 3125, 125):\n",
    "      results_P = {}\n",
    "      CONFIG.spectral_threshold = P\n",
    "      for N in [84,]:#range(4, 324, 80):\n",
    "        CONFIG.dbn_hidden_layer_sizes = [N, N, N]\n",
    "\n",
    "        mat_c_all, mat_x_all, nonempty, representatives = compress_data(\n",
    "            mat_q.abs(), CONFIG.read_period, CONFIG.test_period, CONFIG.alpha)\n",
    "        mat_q_trend_all, mat_q_resid_all = preprocess_data(\n",
    "            CONFIG.spectral_threshold, mat_c_all)\n",
    "        mat_q_resid_all = crop_q_between(mat_q_resid_all, CONFIG.read_period,\n",
    "                                         CONFIG.test_period).real\n",
    "        mat_c_all = crop_q_between(\n",
    "            mat_c_all, CONFIG.read_period, CONFIG.test_period)\n",
    "\n",
    "        representatives = torch.tensor([160,])\n",
    "        for section_index, section_number in tqdm(enumerate(representatives.tolist())):\n",
    "          mat_q_trend = mat_q_trend_all[:, section_index][:, None].real\n",
    "          mat_q_resid = mat_q_resid_all[:, section_index][:, None].real\n",
    "\n",
    "          norm_const = normalizing_constants_all[section_number].item()\n",
    "\n",
    "          (_, val_trend_wd_dataloader, _), (_, val_trend_we_dataloader, _) = \\\n",
    "            crop_and_split_mat(mat_q_trend, CONFIG, test_period=True, separate_weekends=False)\n",
    "          # (_, val_resid_wd_dataloader, _), (_, val_resid_we_dataloader, _) =\n",
    "          #   crop_and_split_mat(mat_q_resid, CONFIG)\n",
    "          mat_c_wd_datasets, mat_c_we_datasets = split_mat(mat_q_resid, CONFIG)\n",
    "          # print(gamma, reg_coeff, P, N, mat_c.shape)\n",
    "\n",
    "          del mat_q_resid\n",
    "\n",
    "          # dbn, kelm, val_resid_we_dataloader = train_with_config(\n",
    "          #     CONFIG, mat_c_we_datasets, dbn_training_epochs=100, stride=1,\n",
    "          #     gamma=gamma, reg_coeff=reg_coeff)\n",
    "          # val_resid_we_dataloader = DataLoader(mat_c_we_datasets[1])\n",
    "          val_resid_wd_dataloader = DataLoader(mat_c_wd_datasets[1])\n",
    "          prefix = f'alpha{CONFIG.alpha}_p{P}_n{N}_sec{section_number}_'\\\n",
    "            .replace('.', '_')\n",
    "          dbn = DBN(CONFIG.time_window_length, CONFIG.dbn_hidden_layer_sizes,\n",
    "            CONFIG.gibbs_sampling_steps).to(CONFIG.device)\n",
    "          dbn.load_state_dict(CONFIG.load(prefix + 'dbn.pt'))\n",
    "          \n",
    "          train_len = int(len(mat_c_wd_datasets[1]) * train_portion) - 2\n",
    "          kelm_state = CONFIG.load(prefix + 'kelm.pt')\n",
    "          kelm = KELM(kelm_state['_beta'].shape, kelm_state['_data'].shape).to(CONFIG.device)\n",
    "          kelm.gamma = 1.0\n",
    "          kelm.reg_coeff = 1.0\n",
    "          kelm.load_state_dict(kelm_state)\n",
    "\n",
    "          mse_loss_trend = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          mse_loss_resid = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          mse_loss_overall = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          mse_loss_trend_unnorm = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          mse_loss_resid_unnorm = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          mse_loss_overall_unnorm = torch.tensor([0.,]).to(CONFIG.device)\n",
    "\n",
    "          trend_y = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          resid_y = torch.tensor([0.,]).to(CONFIG.device)\n",
    "          overall_y = torch.tensor([0.,]).to(CONFIG.device)\n",
    "\n",
    "          iter_trend = iter(val_trend_wd_dataloader)\n",
    "          n_samples = 0\n",
    "\n",
    "          resid = {'y': [], 'pred': []}\n",
    "          trend = {'y': [], 'pred': []}\n",
    "          overall = {'y': [], 'pred': []}\n",
    "          for X_resid, y_resid in val_resid_wd_dataloader:\n",
    "              resid_y += y_resid.item()\n",
    "              X_trend, y_trend = next(iter_trend)\n",
    "              trend_y += y_trend.item()\n",
    "              pred_trend = ssa.forecast(\n",
    "                  X_trend.squeeze(0).T, 1).sum(0)[-1][None]\n",
    "              mse_loss_trend += mse(pred_trend, y_trend)\n",
    "              mse_loss_trend_unnorm += mse(pred_trend * norm_const, y_trend * norm_const)\n",
    "              trend['y'].append(y_trend.item() * norm_const)\n",
    "              trend['pred'].append(pred_trend.item() * norm_const)\n",
    "              # print('TREND', y_trend, pred_trend)\n",
    "\n",
    "              pred_resid = dbn(X_resid).squeeze(0)\n",
    "              pred_resid = kelm(pred_resid).T\n",
    "              mse_loss_resid += mse(pred_resid, y_resid)\n",
    "              mse_loss_resid_unnorm += mse(pred_resid * norm_const, y_resid * norm_const)\n",
    "              resid['y'].append(y_resid.item() * norm_const)\n",
    "              resid['pred'].append(pred_resid.item() * norm_const)\n",
    "\n",
    "              # print('RESID', y_resid, pred_resid)\n",
    "\n",
    "              pred = pred_trend + pred_resid\n",
    "\n",
    "              y = y_trend + y_resid\n",
    "              overall_y += y.item()\n",
    "              overall['y'].append(y.item() * norm_const)\n",
    "              overall['pred'].append(pred.item() * norm_const)\n",
    "              # print('OVRLL', y, pred)\n",
    "              # print('=====')\n",
    "\n",
    "              mse_loss_overall += mse(pred, y).item()\n",
    "              mse_loss_overall_unnorm += mse(pred * norm_const, y * norm_const).item()\n",
    "              n_samples += 1\n",
    "\n",
    "          mse_loss_trend /= n_samples\n",
    "          mse_loss_resid /= n_samples\n",
    "          mse_loss_overall /= n_samples\n",
    "          mse_loss_resid_unnorm /= n_samples\n",
    "          mse_loss_trend_unnorm /= n_samples\n",
    "          mse_loss_overall_unnorm /= n_samples\n",
    "\n",
    "          resid_y /= n_samples\n",
    "          trend_y /= n_samples\n",
    "          overall_y /= n_samples\n",
    "\n",
    "          results_P[section_number] = {\n",
    "              'resid': resid,\n",
    "              'trend': trend,\n",
    "              'overall': overall\n",
    "          }\n",
    "\n",
    "          with open(f'results_wd_{section_number}.json', 'w') as file:\n",
    "            dump(results_P[section_number], file)\n",
    "\n",
    "          # print(f'gamma={gamma}, reg_coeff={reg_coeff}, ' +\n",
    "          #       f'WE P={CONFIG.spectral_threshold}, ' +\n",
    "          #       f'N={CONFIG.dbn_hidden_layer_sizes}, ' +\n",
    "          #       f'trend={trend_y.item()}+-{mse_loss_trend.item()}' +\n",
    "          #       f'~{mse_loss_trend_unnorm.item()}, ' +\n",
    "          #       f'resid={resid_y.item()}+-{mse_loss_resid.item()}' +\n",
    "          #       f'~{mse_loss_resid_unnorm.item()}, ' +\n",
    "          #       f'overall={overall_y.item()}+-{mse_loss_overall.item()}' +\n",
    "          #       f'~{mse_loss_overall_unnorm.item()}')\n",
    "          del dbn\n",
    "          del kelm\n",
    "      results_wd[P] = results_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dump\n",
    "with open('results_we_16.json', 'w') as file:\n",
    "    dump(results_P, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
